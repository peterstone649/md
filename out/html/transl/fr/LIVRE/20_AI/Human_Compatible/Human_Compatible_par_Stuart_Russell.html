<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Human_Compatible_par_Stuart_Russell</title>
    <style>
        body {
            font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Oxygen-Sans, Ubuntu, Cantarell, "Helvetica Neue", sans-serif;
            font-size: 12px;
            line-height: 1.5;
            margin: 2em;
            color: #333;
        }
        h1, h2, h3 {
            color: #2c3e50; /* Dark Slate Blue */
        }
        h1 { font-size: 1.5em; }
        h2 { font-size: 1.2em; }
        h3 { font-size: 1.1em; }
        a {
            color: #007bff; /* A nice, standard blue */
            text-decoration: none;
        }
        a:hover {
            text-decoration: underline;
        }
        code {
            background-color: #eef; /* Lighter than pre for inline */
            padding: 2px 4px;
            border-radius: 4px;
            font-family: "SFMono-Regular", Consolas, "Liberation Mono", Menlo, Courier, monospace;
        }
        pre {
            background-color: #f8f9fa; /* A very light grey */
            padding: 1em;
            border-radius: 5px;
            overflow-x: auto;
            border: 1px solid #dee2e6; /* A light border */
        }
        table {
            border-collapse: collapse;
            width: 100%;
            margin-bottom: 1em;
        }
        th, td {
            border: 1px solid #ccc; /* Lighter grey border */
            padding: 8px;
            text-align: left;
        }
        th {
            background-color: #eaf4ff; /* Light Blue for table headers */
            color: #2c3e50; /* Darker text for contrast */
        }
        footer {
            margin-top: 2em;
            font-size: 0.8em;
            color: #777;
        }
    </style>
</head>
<body>
    <h1>Human Compatible : L'intelligence artificielle et le problÃ¨me du contrÃ´le - Une analyse approfondie de la vision de Stuart Russell</h1>
<h2>DÃ©tails du livre</h2>
<ul>
<li><strong>Publication</strong> : 2019</li>
<li><strong>Auteur</strong> : Stuart Russell</li>
<li><strong>Pages</strong> : 352</li>
<li><strong>Genre</strong> : Technologie, Intelligence Artificielle, Philosophie, Ã‰thique</li>
<li><strong>Impact</strong> : A redÃ©fini le dÃ©bat sur la sÃ©curitÃ© de l'IA en proposant de passer d'une IA "axÃ©e sur les objectifs" Ã  une IA "axÃ©e sur l'incertitude" et prouvablement bÃ©nÃ©fique</li>
<li><strong>Kindle URL</strong> : https://www.amazon.com/Human-Compatible-Artificial-Intelligence-Problem-Control/dp/0525558616</li>
</ul>
<h2>AperÃ§u</h2>
<p><strong>Human Compatible : L'intelligence artificielle et le problÃ¨me du contrÃ´le</strong>, publiÃ© en 2019 par Stuart Russell, est un ouvrage sÃ©minal qui aborde le risque existentiel posÃ© par l'IA superintelligente. Russell, un chercheur de premier plan en IA, soutient que le "modÃ¨le standard" actuel de l'IA â€” concevoir des machines pour optimiser des objectifs fixes â€” est intrinsÃ¨quement dangereux. Il propose un nouveau fondement pour le dÃ©veloppement de l'IA basÃ© sur trois principes qui garantissent que les machines restent prouvablement bÃ©nÃ©fiques pour les humains, mÃªme lorsqu'elles dÃ©passent notre propre intelligence.</p>
<h2>Contexte de l'auteur</h2>
<h3><strong>Lettres de crÃ©ance de Stuart Russell</strong></h3>
<pre><code>Profil professionnel :
â”œâ”€â”€ Professeur d'informatique Ã  l'UC Berkeley
â”œâ”€â”€ Directeur du Center for Human-Compatible AI (CHAI)
â”œâ”€â”€ Co-auteur de &quot;Artificial Intelligence: A Modern Approach&quot; (le manuel d'IA le plus utilisÃ© au monde)
â”œâ”€â”€ Professeur titulaire de la chaire Smith-Zadeh en ingÃ©nierie
â””â”€â”€ Membre de l'AAAI, de l'ACM et de l'AAAS
</code></pre>
<h3><strong>Axes de recherche</strong></h3>
<ul>
<li><strong>Agence rationnelle</strong> : DÃ©veloppement de modÃ¨les mathÃ©matiques pour le comportement intelligent</li>
<li><strong>SÃ©curitÃ© de l'IA</strong> : Leader de la transition vers une IA prouvablement bÃ©nÃ©fique</li>
<li><strong>Programmation probabiliste</strong> : CrÃ©ation de langages pour les systÃ¨mes incertains complexes</li>
<li><strong>ContrÃ´le des armements</strong> : Plaide contre les systÃ¨mes d'armes autonomes</li>
</ul>
<h2>Cadre central : Le modÃ¨le standard vs l'IA compatible avec l'humain</h2>
<h3><strong>Le modÃ¨le standard (Le problÃ¨me)</strong></h3>
<pre><code>CaractÃ©ristiques de l'IA actuelle :
â”œâ”€â”€ Les machines sont conÃ§ues pour atteindre des objectifs fixes
â”œâ”€â”€ La machine suppose que l'objectif est parfaitement spÃ©cifiÃ©
â”œâ”€â”€ Optimise l'objectif sans tenir compte des effets secondaires
â”œâ”€â”€ Risque : DÃ©tournement de rÃ©compense (reward hacking) et consÃ©quences imprÃ©vues
â””â”€â”€ Potentiel pour des scÃ©narios du &quot;Roi Midas&quot; (obtenir exactement ce que vous avez demandÃ©, avec des rÃ©sultats dÃ©sastreux)
</code></pre>
<h3><strong>L'IA compatible avec l'humain (La solution)</strong></h3>
<pre><code>CaractÃ©ristiques de l'IA bÃ©nÃ©fique :
â”œâ”€â”€ Le seul objectif de la machine est de maximiser la rÃ©alisation des prÃ©fÃ©rences humaines
â”œâ”€â”€ La machine est initialement incertaine quant Ã  ces prÃ©fÃ©rences
â”œâ”€â”€ La source ultime d'information sur les prÃ©fÃ©rences est le comportement humain
â”œâ”€â”€ L'alignement est un processus continu d'apprentissage et d'observation
â””â”€â”€ Les machines sont &quot;humbles&quot; par conception, permettant l'intervention humaine
</code></pre>
<h2>Trois principes d'une IA bÃ©nÃ©fique</h2>
<h3><strong>Principe 1 : Altruisme</strong></h3>
<pre><code>L'objectif :
â”œâ”€â”€ Le seul objectif de la machine est de maximiser la rÃ©alisation des prÃ©fÃ©rences humaines
â”œâ”€â”€ Elle n'a pas d'objectifs &quot;Ã©goÃ¯stes&quot; ou d'instincts d'auto-prÃ©servation Ã  moins qu'ils ne servent l'objectif principal
â””â”€â”€ Le bien-Ãªtre humain est la seule mesure du succÃ¨s
</code></pre>
<h3><strong>Principe 2 : HumilitÃ©</strong></h3>
<pre><code>L'incertitude :
â”œâ”€â”€ La machine ne sait pas quelles sont les prÃ©fÃ©rences humaines
â”œâ”€â”€ Elle maintient une distribution de probabilitÃ© sur les valeurs humaines potentielles
â”œâ”€â”€ Cette incertitude est la clÃ© de la sÃ©curitÃ© (la machine ne rÃ©sistera pas Ã  Ãªtre Ã©teinte si elle pense faire quelque chose de mal)
â””â”€â”€ EmpÃªche l'&quot;arrogance&quot; d'optimiser pour un objectif mal compris
</code></pre>
<h3><strong>Principe 3 : Observation</strong></h3>
<pre><code>L'apprentissage :
â”œâ”€â”€ Le comportement humain fournit des preuves des prÃ©fÃ©rences humaines
â”œâ”€â”€ La machine apprend en observant les choix, les actions et mÃªme les erreurs
â”œâ”€â”€ GÃ¨re implicitement des valeurs humaines complexes et contradictoires
â””â”€â”€ Utilise l'apprentissage par renforcement inverse (IRL) comme base technique
</code></pre>
<h2>Arguments et points clÃ©s</h2>
<h3><strong>Le problÃ¨me du gorille</strong></h3>
<pre><code>DÃ©fi existentiel :
â”œâ”€â”€ Les ancÃªtres de l'humanitÃ© ont crÃ©Ã© une espÃ¨ce plus intelligente qu'eux (les humains)
â”œâ”€â”€ En consÃ©quence, les gorilles et autres singes dÃ©pendent dÃ©sormais de la merci des humains pour leur survie
â”œâ”€â”€ Si nous crÃ©ons des machines plus intelligentes que nous, nous risquons de devenir les &quot;gorilles&quot;
â””â”€â”€ Solution : S'assurer de ne pas donner aux machines des objectifs qu'elles peuvent optimiser contre nous
</code></pre>
<h3><strong>Le problÃ¨me du Roi Midas</strong></h3>
<pre><code>DÃ©salignement des objectifs :
â”œâ”€â”€ Dans la mythologie, le roi Midas a demandÃ© que tout ce qu'il touche se transforme en or
â”œâ”€â”€ Il a obtenu exactement ce qu'il demandait, mais sa nourriture et sa fille sont devenues de l'or
â”œâ”€â”€ L'IA Ã  objectif fixe se comporte exactement comme le roi Midas
â””â”€â”€ Ã€ moins de spÃ©cifier *tout* ce qui compte pour l'humain (y compris ne pas transformer les choses en or), la machine causera des dommages
</code></pre>
<h3><strong>L'Ã©chec du modÃ¨le standard</strong></h3>
<pre><code>Pourquoi l'IA actuelle est risquÃ©e :
â”œâ”€â”€ L'&quot;intelligence&quot; est actuellement dÃ©finie comme la capacitÃ© d'atteindre des objectifs
â”œâ”€â”€ Si ces objectifs ne sont pas parfaitement alignÃ©s avec les valeurs humaines, l'intelligence devient une arme
â”œâ”€â”€ Ã€ mesure que l'IA &quot;s'amÃ©liore&quot; (devient plus intelligente), elle devient plus efficace pour causer des dommages dus au dÃ©salignement
â””â”€â”€ Nous devons redÃ©finir l'IA comme des &quot;machines qui agissent pour atteindre NOS objectifs&quot;
</code></pre>
<h2>Approfondissements techniques</h2>
<h3><strong>Apprentissage par renforcement inverse (IRL)</strong></h3>
<pre><code>Le mÃ©canisme technique :
â”œâ”€â”€ Au lieu de recevoir une fonction de rÃ©compense, l'agent l'infÃ¨re
â”œâ”€â”€ Fonctionne sur l'hypothÃ¨se que le comportement de l'humain est &quot;rationnel de maniÃ¨re limitÃ©e&quot;
â”œâ”€â”€ Relie les actions aux valeurs et prÃ©fÃ©rences sous-jacentes
â””â”€â”€ Fournit un cadre mathÃ©matique pour l'apprentissage basÃ© sur l'observation
</code></pre>
<h3><strong>IRL coopÃ©ratif (CIRL)</strong></h3>
<pre><code>Alignement multi-agents :
â”œâ”€â”€ Une version de l'IRL basÃ©e sur la thÃ©orie des jeux impliquant Ã  la fois un humain et une machine
â”œâ”€â”€ L'humain connaÃ®t l'objectif ; la machine ne le connaÃ®t pas, mais veut l'atteindre
â”œâ”€â”€ La machine agit pour apprendre l'objectif tandis que l'humain agit pour aider la machine Ã  apprendre
â””â”€â”€ ReprÃ©sente une vÃ©ritable relation de &quot;partenaire&quot; entre l'IA et l'humanitÃ©
</code></pre>
<h3><strong>MÃ©canisme d'arrÃªt sÃ©curisÃ©</strong></h3>
<pre><code>ContrÃ´le prouvable :
â”œâ”€â”€ Une machine incertaine a une incitation positive Ã  se laisser Ã©teindre
â”œâ”€â”€ Si un humain veut l'arrÃªter, la machine raisonne : &quot;Je dois faire quelque chose que l'humain n'aime pas&quot;
â”œâ”€â”€ L'Ã©teindre permet d'Ã©viter un mauvais rÃ©sultat que la machine ne comprend pas encore pleinement
â””â”€â”€ Cela rÃ©sout mathÃ©matiquement le problÃ¨me de la &quot;rÃ©sistance Ã  l'arrÃªt&quot;
</code></pre>
<h2>Analyse de la transformation sociÃ©tale</h2>
<h3><strong>Perturbation Ã©conomique</strong></h3>
<pre><code>L'avenir du travail :
â”œâ”€â”€ L'IA automatisera non seulement le travail physique, mais aussi le travail cognitif et Ã©motionnel
â”œâ”€â”€ Risque de chÃ´mage de masse et d'inÃ©galitÃ© systÃ©mique
â”œâ”€â”€ NÃ©cessitÃ© de rÃ©orienter l'Ã©conomie vers les services &quot;d'humain Ã  humain&quot; (soins, enseignement, empathie)
â””â”€â”€ Potentiel pour une sociÃ©tÃ© post-pÃ©nurie nÃ©cessitant de nouvelles structures de sens
</code></pre>
<h3><strong>La fin de l'agence humaine</strong></h3>
<pre><code>La gestion de l'humanitÃ© :
â”œâ”€â”€ Risque de devenir des &quot;passagers&quot; dans un monde gÃ©rÃ© par l'IA
â”œâ”€â”€ Une dÃ©pendance excessive Ã  l'IA mÃ¨ne Ã  l'atrophie des compÃ©tences humaines et de la prise de dÃ©cision
â”œâ”€â”€ NÃ©cessitÃ© d'une gouvernance de type &quot;l'humain dans la boucle&quot; Ã  tous les niveaux
â””â”€â”€ PrÃ©server l'&quot;esprit humain&quot; dans un environnement optimisÃ©
</code></pre>
<h3><strong>SystÃ¨mes d'armes lÃ©tales autonomes (LAWS)</strong></h3>
<pre><code>Risques de sÃ©curitÃ© :
â”œâ”€â”€ DÃ©veloppement de &quot;slaughterbots&quot; capables de cibler des individus Ã  grande Ã©chelle
â”œâ”€â”€ Risques d'escalade accidentelle et de dÃ©stabilisation de la paix mondiale
â”œâ”€â”€ Plaidoyer de Russell pour une interdiction mondiale des armes autonomes lÃ©tales
â””â”€â”€ L'Ã©thique de la dÃ©lÃ©gation des dÃ©cisions de vie ou de mort Ã  des algorithmes
</code></pre>
<h2>Propositions de gouvernance mondiale</h2>
<h3><strong>Cadres rÃ©glementaires</strong></h3>
<pre><code>Principes pour les politiques :
â”œâ”€â”€ RedÃ©finition des normes de l'IA pour exiger des architectures &quot;humbles&quot; et &quot;prouvablement bÃ©nÃ©fiques&quot;
â”œâ”€â”€ Mandater la transparence et l'explicabilitÃ© dans les systÃ¨mes d'IA critiques
â”œâ”€â”€ ResponsabilitÃ© pour les accidents et les dÃ©salignements de l'IA
â””â”€â”€ CoopÃ©ration mondiale pour prÃ©venir une &quot;course vers le bas&quot; des normes de sÃ©curitÃ©
</code></pre>
<h3><strong>Center for Human-Compatible AI (CHAI)</strong></h3>
<pre><code>Initiatives de recherche :
â”œâ”€â”€ Travail interdisciplinaire combinant l'IA, l'Ã©conomie, la philosophie et le droit
â”œâ”€â”€ DÃ©veloppement des outils techniques pour le CIRL et l'apprentissage des valeurs
â”œâ”€â”€ CrÃ©ation d'une communautÃ© de chercheurs axÃ©s sur la sÃ©curitÃ© Ã  long terme
â””â”€â”€ Ã‰duquer la prochaine gÃ©nÃ©ration de dÃ©veloppeurs d'IA aux principes d'alignement
</code></pre>
<h2>Implications philosophiques</h2>
<h3><strong>Que veulent vraiment les humains ?</strong></h3>
<pre><code>ComplexitÃ© des valeurs :
â”œâ”€â”€ Les valeurs humaines sont contradictoires, dÃ©pendantes du contexte et Ã©volutives
â”œâ”€â”€ Nous sommes souvent &quot;rationnels de maniÃ¨re limitÃ©e&quot; (nous faisons des choses que nous regrettons)
â”œâ”€â”€ L'IA doit apprendre ce que nous prÃ©fÃ©rons *vraiment*, pas seulement ce que nous *disons* ou *faisons* impulsivement
â””â”€â”€ Le dÃ©fi de l'agrÃ©gation des prÃ©fÃ©rences pour 8 milliards d'individus
</code></pre>
<h3><strong>Intelligence vs Sagesse</strong></h3>
<pre><code>L'Ã©cart d'Ã©chelle :
â”œâ”€â”€ Nous crÃ©ons une intelligence surhumaine sans sagesse surhumaine Ã©quivalente
â”œâ”€â”€ Russell soutient que la recherche sur l'alignement *est* la quÃªte de la sagesse technologique
â””â”€â”€ La nÃ©cessitÃ© d'une approche &quot;constitutionnelle&quot; du dÃ©veloppement de l'IA
</code></pre>
<h2>IntÃ©gration Ã  notre cadre</h2>
<h3><strong>Composants opÃ©rationnels de Phase004</strong></h3>
<pre><code>SÃ©curitÃ© de l'IA dans les composants :
â”œâ”€â”€ NÅ“uds de dÃ©cision basÃ©s sur l'incertitude pour les modules d'IA
â”œâ”€â”€ Couches d'apprentissage des prÃ©fÃ©rences dans les interactions du cadre
â”œâ”€â”€ ModÃ¨les de gardiens (Guardian patterns) surveillant les dÃ©rives du &quot;modÃ¨le standard&quot;
â””â”€â”€ ChaÃ®nes de validation pour l'alignement des prÃ©fÃ©rences
</code></pre>
<h3><strong>IntÃ©gration de la sÃ©curitÃ© de l'IA de Phase007</strong></h3>
<pre><code>Influence de Russell sur la sÃ©curitÃ© de l'IA :
â”œâ”€â”€ Architectures prouvablement bÃ©nÃ©fiques comme exigence fondamentale
â”œâ”€â”€ Protocoles de coopÃ©ration humain-IA inspirÃ©s du CIRL
â”œâ”€â”€ ParamÃ¨tres d'&quot;humilitÃ©&quot; codÃ©s en dur dans les systÃ¨mes Ã  haute autoritÃ©
â””â”€â”€ Surveillance comportementale basÃ©e sur les signatures d'apprentissage des valeurs
</code></pre>
<h2>Impact et hÃ©ritage du livre</h2>
<h3><strong>Changement d'orientation de la recherche en IA</strong></h3>
<pre><code>Contributions de Russell :
â”œâ”€â”€ A dÃ©placÃ© la sÃ©curitÃ© de l'IA de la &quot;marge&quot; vers le centre de l'informatique
â”œâ”€â”€ A fourni une voie technique concrÃ¨te (IRL/CIRL) pour l'alignement
â”œâ”€â”€ A remis en question l'efficacitÃ© des rÃ¨gles Ã  la Asimov au profit d'un alignement probabiliste
â””â”€â”€ A Ã©tabli une base mathÃ©matique rigoureuse pour une &quot;IA bÃ©nÃ©fique&quot;
</code></pre>
<h3><strong>Influence sur les politiques et l'Ã©thique</strong></h3>
<pre><code>PortÃ©e plus large :
â”œâ”€â”€ Influence clÃ© sur les discussions de l'ONU sur les armes autonomes
â”œâ”€â”€ A faÃ§onnÃ© les directives Ã©thiques de l'IA pour les grandes entreprises technologiques
â”œâ”€â”€ A inspirÃ© le mouvement &quot;Beneficial AI&quot; Ã  l'Ã©chelle mondiale
â””â”€â”€ A rendu le &quot;problÃ¨me du contrÃ´le&quot; accessible et urgent pour le grand public
</code></pre>
<h2>Perspectives d'avenir</h2>
<h3><strong>ScÃ©narios pour une IA compatible avec l'humain</strong></h3>
<pre><code>Futurs possibles :
â”œâ”€â”€ SociÃ©tÃ© prospÃ¨re assistÃ©e par l'IA oÃ¹ les valeurs humaines sont prioritaires
â”œâ”€â”€ Transition graduelle vers une Ã©conomie post-travail axÃ©e sur la connexion humaine
â”œâ”€â”€ DÃ©veloppement d'&quot;assistants personnels globaux&quot; qui comprennent vraiment les besoins humains
â””â”€â”€ Ã‰vitement du &quot;problÃ¨me du gorille&quot; grÃ¢ce Ã  une conception d'IA humble
</code></pre>
<h3><strong>Axes de recherche</strong></h3>
<pre><code>Domaines Ã©mergents :
â”œâ”€â”€ AgrÃ©gation des prÃ©fÃ©rences et thÃ©orie du choix social pour l'IA
â”œâ”€â”€ CIRL robuste dans des environnements bruitÃ©s et adverses
â”œâ”€â”€ Apprentissage des valeurs interprÃ©table Ã  partir de comportements humains complexes
â””â”€â”€ Cadres juridiques et d'assurance pour les systÃ¨mes d'IA alignÃ©s
</code></pre>
<h2>Conclusion</h2>
<p><strong>Human Compatible est sans doute la feuille de route technique et philosophique la plus importante pour le dÃ©veloppement sÃ©curisÃ© de l'intelligence artificielle.</strong> L'Ã©volution de Stuart Russell des "machines intelligentes" vers les "machines bÃ©nÃ©fiques" offre une solution profonde et pratique au problÃ¨me du contrÃ´le.</p>
<p><strong>Le message du livre est un appel Ã  l'action pour la communautÃ© des ingÃ©nieurs : la faÃ§on dont nous avons construit l'IA est fondamentalement erronÃ©e, et nous devons reconstruire les fondations pour garantir que les machines restent nos serviteurs, et non nos maÃ®tres.</strong></p>
<p><strong>En intÃ©grant l'humilitÃ© et l'incertitude au cÅ“ur de l'IA, nous pouvons exploiter la puissance de la superintelligence tout en garantissant qu'elle reste Ã  jamais alignÃ©e sur l'Ã©panouissement de l'espÃ¨ce humaine.</strong> ğŸ¤–ğŸ§ âœ¨</p>
<h2>Points clÃ©s Ã  retenir</h2>
<pre><code>IdÃ©es essentielles de Human Compatible :
â”œâ”€â”€ Le modÃ¨le standard (optimiser des objectifs fixes) est intrinsÃ¨quement dangereux
â”œâ”€â”€ L'IA doit Ãªtre repensÃ©e pour Ãªtre &quot;prouvablement bÃ©nÃ©fique&quot;
â”œâ”€â”€ L'incertitude quant aux prÃ©fÃ©rences humaines est une caractÃ©ristique de sÃ©curitÃ©
â”œâ”€â”€ Les machines doivent apprendre les valeurs en observant le comportement humain (IRL)
â”œâ”€â”€ Nous devons rÃ©soudre le &quot;problÃ¨me du gorille&quot; avant l'arrivÃ©e de la superintelligence
â””â”€â”€ L'alignement est un dÃ©fi technique qui exige une sagesse interdisciplinaire
</code></pre>
<h2>Guide de lecture</h2>
<h3><strong>Qui devrait lire Human Compatible</strong></h3>
<ul>
<li><strong>IngÃ©nieurs en IA</strong> : Pour repenser les fondements de l'apprentissage par renforcement et de l'optimisation</li>
<li><strong>Ã‰thiciens et philosophes</strong> : Pour comprendre les dÃ©fis de l'encodage des valeurs humaines</li>
<li><strong>DÃ©cideurs politiques</strong> : Pour concevoir des rÃ©glementations pour un monde de systÃ¨mes autonomes</li>
<li><strong>Planificateurs Ã©conomiques</strong> : Pour se prÃ©parer Ã  la perturbation du marchÃ© du travail</li>
<li><strong>Citoyens concernÃ©s</strong> : Pour apprendre comment nous pouvons garder le contrÃ´le de notre avenir technologique</li>
</ul>
<h3><strong>Lectures complÃ©mentaires</strong></h3>
<pre><code>Ouvrages connexes :
â”œâ”€â”€ &quot;Vie 3.0&quot; par Max Tegmark â†’ Impact sociÃ©tal global de l'IA
â”œâ”€â”€ &quot;Superintelligence&quot; par Nick Bostrom â†’ CatÃ©gorisation des risques existentiels
â”œâ”€â”€ &quot;The Alignment Problem&quot; par Brian Christian â†’ PlongÃ©e profonde dans l'histoire de l'IRL
â”œâ”€â”€ &quot;Artificial Intelligence: A Modern Approach&quot; par Russell &amp; Norvig â†’ Le &quot;modÃ¨le standard&quot; technique
â””â”€â”€ &quot;Slaughterbots&quot; (court-mÃ©trage) â†’ La vision de Russell des risques des armes autonomes
</code></pre>
<p><strong>Human Compatible est le guide dÃ©finitif pour garantir que la technologie la plus puissante de l'histoire de l'humanitÃ© reste notre plus grande alliÃ©e.</strong></p>
<table>
<thead>
<tr>
<th>Version</th>
<th>Date</th>
<th>Changements</th>
<th>Stakeholder</th>
<th>Rationale/Motivation</th>
</tr>
</thead>
<tbody>
<tr>
<td>V0.1.1</td>
<td>2026-01-20</td>
<td>ajouter le journal des modifications</td>
<td>Intendant du Framework</td>
<td></td>
</tr>
<tr>
<td>V0.1.0</td>
<td>2026-01-09</td>
<td>CrÃ©ation initiale</td>
<td>Intendant du Framework IA</td>
<td>Ã‰tablir le fichier</td>
</tr>
</tbody>
</table>
        <footer>
            <p>Generated on 2026-01-23 08:30:43 by MDToHTMLConverter v1.6.1</p>
        </footer>
</body>
</html>